
##### Lab 1: Implementation of Perceptron Learning on AND Logic Gate Function

##### Lab 2: Implementation of Offline Learning or Batch Perceptron Learning on OR Logic Gate Function

##### Lab 3:
###### Consider X-OR Learning Problem by MLP. Find the final weight of the MLP by using backpropagation algorithm for training.
###### Use activation function sigmoid, tanh and ReLu comparing which one will find the final weight.
###### learning rate = 0.001
##### bias = 1

###### Input Layer weights | assume inputs x self
###### w00 = 0.7
###### w01 = 0.6
###### w02 0 0.8

###### w10 = 0.4
###### w11 = 0.8
###### w12 = 0.2

###### Hidden Layers Weights
###### W00 = 0.4
###### W10 = 0.7
###### W20 = 0.8

##### Lab 4: Implement Linear Regression y = mx + b and Multiple Linear Regression y = bo + b1x1+ b2x2 + ... + bnxn using gradient descent method (indirect method/iterative method) using batch training/learning.

