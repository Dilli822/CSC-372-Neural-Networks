
print(
"""
### ğŸ“Œ Mathematics of Gradient Descent & Backpropagation for XOR MLP

#### **1ï¸âƒ£ Forward Propagation Equations**

1. **Hidden Layer Calculations** - Compute weighted sum of inputs:
      ```
      H = X â‹… W_input_hidden + B_hidden
      ```
    - Apply activation function:
      ```
      H_activated = f(H)
      ```

2. **Output Layer Calculations** - Compute weighted sum of hidden layer outputs:
      ```
      O = H_activated â‹… W_hidden_output + B_output
      ```
    - Apply activation function (Sigmoid or Softmax):
      ```
      O_activated = g(O)
      ```

---

#### **2ï¸âƒ£ Compute Error (Loss Function)**

1. **Mean Absolute Error (MAE):** ```
    Loss = (1/N) Î£ |y - O_activated|
    ```
    or **Mean Squared Error (MSE):** ```
    Loss = (1/N) Î£ (y - O_activated)Â²
    ```

---

#### **3ï¸âƒ£ Backpropagation: Compute Gradients**

1. **Compute Error at Output Layer** ```
    Error = y - O_activated
    ```

2. **Compute Gradient for Output Layer (If Using Sigmoid)** - Derivative of Sigmoid activation:
      ```
      g'(O) = O_activated â‹… (1 - O_activated)
      ```
    - Gradient for output layer:
      ```
      Î´_output = Error â‹… g'(O)
      ```

3. **Compute Gradient for Hidden Layer** - Backpropagate Error to Hidden Layer:
      ```
      Hidden Error = Î´_output â‹… W_hidden_outputáµ€
      ```
    - Compute Gradient for Hidden Layer:
      ```
      Î´_hidden = Hidden Error â‹… f'(H_activated)
      ```

---

#### **4ï¸âƒ£ Update Weights & Biases Using Gradient Descent**

1. **Update Weights (Gradient Descent Rule)** - Hidden â†’ Output Layer:
      ```
      W_hidden_output = W_hidden_output + Î· â‹… H_activatedáµ€ â‹… Î´_output
      ```
    - Input â†’ Hidden Layer:
      ```
      W_input_hidden = W_input_hidden + Î· â‹… Xáµ€ â‹… Î´_hidden
      ```

2. **Update Biases** - Output Bias:
      ```
      B_output = B_output + Î· â‹… Î£ Î´_output
      ```
    - Hidden Bias:
      ```
      B_hidden = B_hidden + Î· â‹… Î£ Î´_hidden
      ```

where:
- W = Weight matrices
- B = Biases
- Î· = Learning rate
- X = Input data
- H = Hidden layer activations
- O = Output layer activations
- Î´ = Gradients for weight updates

---

### **ğŸš€ Summary**

âœ… **Forward propagation** computes activations using weights and biases. 
âœ… **Loss function** measures the difference between predictions and actual values. 
âœ… **Backpropagation** computes gradients using the chain rule. 
âœ… **Gradient descent** updates weights and biases to minimize error.
"""
)


print(
    
    """
### ğŸ“Œ Mathematics of Gradient Descent & Backpropagation for XOR MLP

#### **1ï¸âƒ£ Forward Propagation Equations**

1. **Hidden Layer Calculations**  
   - Compute weighted sum of inputs:  
     H = X â‹… W_input-hidden + B_hidden
   - Apply activation function:  
     H_activated = f(H)

2. **Output Layer Calculations**  
   - Compute weighted sum of hidden layer outputs:  
     O = H_activated â‹… W_hidden-output + B_output
   - Apply activation function (Sigmoid or Softmax):  
     O_activated = g(O)

---

#### **2ï¸âƒ£ Compute Error (Loss Function)**

1. **Mean Absolute Error (MAE):**  
   Loss = (1/N) Î£ |y - O_activated|
   or **Mean Squared Error (MSE):**  
   Loss = (1/N) Î£ (y - O_activated)Â²

---

#### **3ï¸âƒ£ Backpropagation: Compute Gradients**

1. **Compute Error at Output Layer**  
   Error = y - O_activated

2. **Compute Gradient for Output Layer (If Using Sigmoid)**  
   - Derivative of Sigmoid activation:  
     g'(O) = O_activated â‹… (1 - O_activated)
   - Gradient for output layer:  
     Î´_output = Error â‹… g'(O)

3. **Compute Gradient for Hidden Layer**  
   - Backpropagate Error to Hidden Layer:  
     Hidden Error = Î´_output â‹… W_hidden-outputáµ€
   - Compute Gradient for Hidden Layer:  
     Î´_hidden = Hidden Error â‹… f'(H_activated)

---

#### **4ï¸âƒ£ Update Weights & Biases Using Gradient Descent**

1. **Update Weights (Gradient Descent Rule)**  
   - Hidden â†’ Output Layer:  
     W_hidden-output = W_hidden-output + Î· â‹… H_activatedáµ€ â‹… Î´_output
   - Input â†’ Hidden Layer:  
     W_input-hidden = W_input-hidden + Î· â‹… Xáµ€ â‹… Î´_hidden

2. **Update Biases**  
   - Output Bias:  
     B_output = B_output + Î· â‹… Î£ Î´_output
   - Hidden Bias:  
     B_hidden = B_hidden + Î· â‹… Î£ Î´_hidden

where:
- W = Weight matrices
- B = Biases
- Î· = Learning rate
- X = Input data
- H = Hidden layer activations
- O = Output layer activations
- Î´ = Gradients for weight updates

---

### **ğŸš€ Summary**

âœ… **Forward propagation** computes activations using weights and biases.  
âœ… **Loss function** measures the difference between predictions and actual values.  
âœ… **Backpropagation** computes gradients using the chain rule.  
âœ… **Gradient descent** updates weights and biases to minimize error.
    """
)